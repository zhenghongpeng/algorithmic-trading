{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Forecast Model - Daily\n",
    "\n",
    "In this notebook we'll train a deep learning model that learns if a target price or stop loss would be hit for a long/short trade in the next days based on historical price data.\n",
    "\n",
    "Model:\n",
    "* Multilayer Perceptron (MLP) (Feedforward neural network)\n",
    "* 3 layers: input, hidden, output\n",
    "* Binary Classification\n",
    "* `Input`: Close, SMA(2 to 16), ROC(2 to 16)\n",
    "* `Output`: Does a long or short trade hit the profit target (2%) without hitting a stop loss (1.5%) in the next five days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_long_short_predict\n"
     ]
    }
   ],
   "source": [
    "%run ../2_Strategies/init_model.py 'model_long_short_predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local/model_long_short_predict/input/config/hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile local/{model_name}/input/config/hyperparameters.json\n",
    "{ \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1) Get Data from Athena and S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'algotrading-s3bucket-azmm1l7vqnf6'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get S3 bucket\n",
    "s3bucket=!(aws s3 ls | grep algotrading- | awk  '{print $3}')\n",
    "s3bucket=s3bucket[0]\n",
    "s3bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyAthena in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.3.0)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from PyAthena) (1.17.100)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from PyAthena) (1.20.100)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from PyAthena) (8.0.1)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.4.4->PyAthena) (0.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.4.4->PyAthena) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore>=1.5.52->PyAthena) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore>=1.5.52->PyAthena) (1.26.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.5.52->PyAthena) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import json\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sage.Session()\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sym</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-13</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.76</td>\n",
       "      <td>26.83</td>\n",
       "      <td>26.41</td>\n",
       "      <td>26.69</td>\n",
       "      <td>23623918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-14</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.80</td>\n",
       "      <td>26.81</td>\n",
       "      <td>26.38</td>\n",
       "      <td>26.48</td>\n",
       "      <td>27477260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-15</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.23</td>\n",
       "      <td>26.47</td>\n",
       "      <td>26.19</td>\n",
       "      <td>26.27</td>\n",
       "      <td>26081909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-16</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.44</td>\n",
       "      <td>26.65</td>\n",
       "      <td>26.34</td>\n",
       "      <td>26.59</td>\n",
       "      <td>25702363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-17</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.57</td>\n",
       "      <td>26.63</td>\n",
       "      <td>26.21</td>\n",
       "      <td>26.33</td>\n",
       "      <td>30379903.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sym   open   high    low  close         vol\n",
       "dt                                                      \n",
       "2012-08-13  INTC  26.76  26.83  26.41  26.69  23623918.0\n",
       "2012-08-14  INTC  26.80  26.81  26.38  26.48  27477260.0\n",
       "2012-08-15  INTC  26.23  26.47  26.19  26.27  26081909.0\n",
       "2012-08-16  INTC  26.44  26.65  26.34  26.59  25702363.0\n",
       "2012-08-17  INTC  26.57  26.63  26.21  26.33  30379903.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyathena import connect\n",
    "conn = connect(s3_staging_dir='s3://'+s3bucket+'/results/',\n",
    "               region_name=region)\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM algo_data.hist_data_daily;\", conn)\n",
    "df.set_index(pd.DatetimeIndex(df['dt']),inplace=True)\n",
    "del df['dt']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=7025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sym</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-13</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.76</td>\n",
       "      <td>26.83</td>\n",
       "      <td>26.41</td>\n",
       "      <td>26.69</td>\n",
       "      <td>23623918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-14</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.80</td>\n",
       "      <td>26.81</td>\n",
       "      <td>26.38</td>\n",
       "      <td>26.48</td>\n",
       "      <td>27477260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-15</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.23</td>\n",
       "      <td>26.47</td>\n",
       "      <td>26.19</td>\n",
       "      <td>26.27</td>\n",
       "      <td>26081909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-16</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.44</td>\n",
       "      <td>26.65</td>\n",
       "      <td>26.34</td>\n",
       "      <td>26.59</td>\n",
       "      <td>25702363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-17</th>\n",
       "      <td>INTC</td>\n",
       "      <td>26.57</td>\n",
       "      <td>26.63</td>\n",
       "      <td>26.21</td>\n",
       "      <td>26.33</td>\n",
       "      <td>30379903.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sym   open   high    low  close         vol\n",
       "dt                                                      \n",
       "2012-08-13  INTC  26.76  26.83  26.41  26.69  23623918.0\n",
       "2012-08-14  INTC  26.80  26.81  26.38  26.48  27477260.0\n",
       "2012-08-15  INTC  26.23  26.47  26.19  26.27  26081909.0\n",
       "2012-08-16  INTC  26.44  26.65  26.34  26.59  25702363.0\n",
       "2012-08-17  INTC  26.57  26.63  26.21  26.33  30379903.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('local/'+model_name+'/input/data/training/data_orig.csv')\n",
    "print(\"count=%s\" % len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff669ba4400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "df[\"close\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) Run Data Preparation Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Data Preparation Code\n",
    "\n",
    "In the following cell, you can modify the data preparation code or leave it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/model_long_short_predict_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/{model_name}_prep.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib as ta\n",
    "from talib.abstract import *\n",
    "import math\n",
    "\n",
    "prefix = '/opt/ml/'\n",
    "input_path = prefix + 'input/data/training'\n",
    "\n",
    "data_orig_file = input_path+'/data_orig.csv'\n",
    "data_file = input_path+'/data.csv'\n",
    "\n",
    "d = pd.read_csv(data_orig_file,infer_datetime_format=True, parse_dates=['dt'], index_col=['dt'])\n",
    "print(d.head())\n",
    "\n",
    "repeatCount=15\n",
    "repeatStep=1\n",
    "lookBack=repeatCount*repeatStep\n",
    "forwardWindow=5\n",
    "\n",
    "profitTarget=2.0/100.0\n",
    "stopTarget=1.5/100.0\n",
    "\n",
    "iCount=lookBack\n",
    "\n",
    "# header\n",
    "hData=[\"dt\"]\n",
    "hData.append(\"close\") \n",
    "for a in range(0,repeatCount):\n",
    "    hData.append(\"wvma\"+str((a+2)*repeatStep))\n",
    "for a in range(0,repeatCount):\n",
    "    hData.append(\"atr\"+str((a+2)*repeatStep))\n",
    "for a in range(0,repeatCount):\n",
    "    hData.append(\"rsi\"+str((a+2)*repeatStep))\n",
    "for a in range(0,repeatCount):\n",
    "    hData.append(\"sma\"+str((a+2)*repeatStep))\n",
    "for a in range(0,repeatCount):\n",
    "    hData.append(\"roc\"+str((a+2)*repeatStep))\n",
    "hData.append(\"long\")\n",
    "hData.append(\"short\")\n",
    "print(hData)\n",
    "\n",
    "# data\n",
    "tData=[]\n",
    "inputs = {\n",
    "    'close': np.array(d[\"close\"]),\n",
    "    'high': np.array(d[\"high\"]),\n",
    "    'low': np.array(d[\"low\"]),\n",
    "    'volume': np.array(d[\"vol\"])\n",
    "}\n",
    "vwma=[]\n",
    "for a in range(0,repeatCount):\n",
    "    vwma.append(WMA(inputs,timeperiod=(a+1)*repeatStep+1))\n",
    "atr=[]\n",
    "for a in range(0,repeatCount):\n",
    "    atr.append(ATR(inputs,timeperiod=(a+1)*repeatStep+1))\n",
    "rsi=[]\n",
    "for a in range(0,repeatCount):\n",
    "    rsi.append(RSI(inputs,timeperiod=(a+1)*repeatStep+1))\n",
    "sma=[]\n",
    "for a in range(0,repeatCount):\n",
    "    sma.append(SMA(inputs,timeperiod=(a+1)*repeatStep+1))\n",
    "roc=[]\n",
    "for a in range(0,repeatCount):\n",
    "    roc.append(ROC(inputs,timeperiod=(a+1)*repeatStep+1))\n",
    "\n",
    "closeList=d[\"close\"]\n",
    "dLen=len(d)\n",
    "n=0\n",
    "lCount=0\n",
    "sCount=0\n",
    "nCount=0\n",
    "n=0\n",
    "for idx,row in d.iterrows():\n",
    "    if n<dLen-forwardWindow-1:\n",
    "        dt1=idx\n",
    "        cl=row[\"close\"]\n",
    "        inputRec=[]\n",
    "        inputRec.append(idx)\n",
    "\n",
    "        inputRec0=[]\n",
    "\n",
    "        #close\n",
    "        inputRec0.append(cl)\n",
    "        #vwma\n",
    "        for a in range(0,repeatCount):\n",
    "            if math.isnan(vwma[a][n]):\n",
    "                inputRec0.append(cl)\n",
    "            else:\n",
    "                inputRec0.append(vwma[a][n])\n",
    "        #atr\n",
    "        for a in range(0,repeatCount):\n",
    "            if math.isnan(atr[a][n]):\n",
    "                inputRec0.append(cl)\n",
    "            else:\n",
    "                inputRec0.append(atr[a][n])\n",
    "                \n",
    "        #rsi\n",
    "        for a in range(0,repeatCount):\n",
    "            if math.isnan(rsi[a][n]):\n",
    "                inputRec0.append(cl)\n",
    "            else:\n",
    "                inputRec0.append(rsi[a][n])\n",
    "        \n",
    "    \n",
    "                \n",
    "        #sma\n",
    "        for a in range(0,repeatCount):\n",
    "            if math.isnan(sma[a][n]):\n",
    "                inputRec0.append(cl)\n",
    "            else:\n",
    "                inputRec0.append(sma[a][n])\n",
    "\n",
    "        m1=min(inputRec0)\n",
    "        m2=max(inputRec0)\n",
    "        for a in inputRec0:\n",
    "            if m2-m1==0:\n",
    "                inputRec.append(0)\n",
    "            else:\n",
    "                inputRec.append((a-m1)/(m2-m1))\n",
    "\n",
    "        #roc\n",
    "        for a in range(0,repeatCount):\n",
    "            if math.isnan(roc[a][n]):\n",
    "                inputRec.append(0)\n",
    "            else:\n",
    "                inputRec.append(roc[a][n])\n",
    "                \n",
    "        rClose=closeList[n+1:min(dLen-1,n+1+forwardWindow)].values.tolist()\n",
    "        low=min(rClose)\n",
    "        high=max(rClose)\n",
    "        \n",
    "            \n",
    "        \n",
    "        #long\n",
    "        long=0\n",
    "        if high>=cl+cl*profitTarget and low>=cl-cl*stopTarget:\n",
    "            long=1\n",
    "            lCount=lCount+1\n",
    "        inputRec.append(long)\n",
    " \n",
    "        #short\n",
    "        short=0\n",
    "        if low<=cl-cl*profitTarget and high<=cl+cl*stopTarget:\n",
    "            short=1\n",
    "            sCount=sCount+1\n",
    "        inputRec.append(short)\n",
    "\n",
    "        tData.append(inputRec)\n",
    "        n=n+1\n",
    "          \n",
    "print(\"lCount=%s,sCount=%s\" % (lCount,sCount))\n",
    "df1=pd.DataFrame(tData,columns=hData)\n",
    "df1.set_index(pd.DatetimeIndex(df1['dt']), inplace=True)\n",
    "del df1['dt']\n",
    " \n",
    "df1.to_csv(data_file)\n",
    "print(df1.head())\n",
    "print(\"count=%s\" % (len(df1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Data Preparation Locally in a Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  29.15MB\n",
      "Step 1/23 : FROM tensorflow/tensorflow:2.1.0rc2-py3\n",
      " ---> 021062082fec\n",
      "Step 2/23 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> c18207f35189\n",
      "Step 3/23 : RUN wget https://sourceforge.net/projects/ta-lib/files/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz && tar -xzf ta-lib-0.4.0-src.tar.gz && cd ta-lib/ && ./configure --prefix=/usr && make && make install && cd ../ && rm -rf ta-lib && rm ta-lib-0.4.0-src.tar.gz\n",
      " ---> Using cache\n",
      " ---> d057f1dca15b\n",
      "Step 4/23 : RUN wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py\n",
      " ---> Using cache\n",
      " ---> 09934ea62a85\n",
      "Step 5/23 : RUN pip install numpy\n",
      " ---> Using cache\n",
      " ---> 009ba349edbd\n",
      "Step 6/23 : RUN pip install scipy\n",
      " ---> Using cache\n",
      " ---> f0185befba4d\n",
      "Step 7/23 : RUN pip install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 2093f8f14efd\n",
      "Step 8/23 : RUN pip install pandas\n",
      " ---> Using cache\n",
      " ---> 138b2b6d5950\n",
      "Step 9/23 : RUN pip install flask\n",
      " ---> Using cache\n",
      " ---> a42de175add4\n",
      "Step 10/23 : RUN pip install gevent\n",
      " ---> Using cache\n",
      " ---> 8b8c4efda03f\n",
      "Step 11/23 : RUN pip install gunicorn\n",
      " ---> Using cache\n",
      " ---> c2c348814126\n",
      "Step 12/23 : RUN pip install tensorflow==2.2.0\n",
      " ---> Using cache\n",
      " ---> d41036bcf9a6\n",
      "Step 13/23 : RUN pip install tensorflow --upgrade\n",
      " ---> Using cache\n",
      " ---> 8184bbc7cf4c\n",
      "Step 14/23 : RUN pip install keras\n",
      " ---> Using cache\n",
      " ---> a9c2f1975416\n",
      "Step 15/23 : RUN pip install backtrader\n",
      " ---> Using cache\n",
      " ---> 467d97a879a0\n",
      "Step 16/23 : RUN pip install matplotlib==3.2.2\n",
      " ---> Using cache\n",
      " ---> 74f3436296f3\n",
      "Step 17/23 : RUN pip install ta-lib\n",
      " ---> Using cache\n",
      " ---> 65c5291307ad\n",
      "Step 18/23 : RUN pip install boto3\n",
      " ---> Using cache\n",
      " ---> cf4d1ef2deba\n",
      "Step 19/23 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 773c89f6e02a\n",
      "Step 20/23 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> b2027e7363d8\n",
      "Step 21/23 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 7ffa1693de58\n",
      "Step 22/23 : COPY model /opt/program\n",
      " ---> Using cache\n",
      " ---> 3740b783dbcb\n",
      "Step 23/23 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> 3f5ffbe3f98a\n",
      "Successfully built 3f5ffbe3f98a\n",
      "Successfully tagged model_long_short_predict_prep:latest\n",
      "             sym   open   high    low  close         vol\n",
      "dt                                                      \n",
      "2012-08-13  INTC  26.76  26.83  26.41  26.69  23623918.0\n",
      "2012-08-14  INTC  26.80  26.81  26.38  26.48  27477260.0\n",
      "2012-08-15  INTC  26.23  26.47  26.19  26.27  26081909.0\n",
      "2012-08-16  INTC  26.44  26.65  26.34  26.59  25702363.0\n",
      "2012-08-17  INTC  26.57  26.63  26.21  26.33  30379903.0\n",
      "['dt', 'close', 'wvma2', 'wvma3', 'wvma4', 'wvma5', 'wvma6', 'wvma7', 'wvma8', 'wvma9', 'wvma10', 'wvma11', 'wvma12', 'wvma13', 'wvma14', 'wvma15', 'wvma16', 'atr2', 'atr3', 'atr4', 'atr5', 'atr6', 'atr7', 'atr8', 'atr9', 'atr10', 'atr11', 'atr12', 'atr13', 'atr14', 'atr15', 'atr16', 'rsi2', 'rsi3', 'rsi4', 'rsi5', 'rsi6', 'rsi7', 'rsi8', 'rsi9', 'rsi10', 'rsi11', 'rsi12', 'rsi13', 'rsi14', 'rsi15', 'rsi16', 'sma2', 'sma3', 'sma4', 'sma5', 'sma6', 'sma7', 'sma8', 'sma9', 'sma10', 'sma11', 'sma12', 'sma13', 'sma14', 'sma15', 'sma16', 'roc2', 'roc3', 'roc4', 'roc5', 'roc6', 'roc7', 'roc8', 'roc9', 'roc10', 'roc11', 'roc12', 'roc13', 'roc14', 'roc15', 'roc16', 'long', 'short']\n",
      "lCount=1633,sCount=1397\n",
      "               close     wvma2     wvma3     wvma4  ...  roc15  roc16  long  short\n",
      "dt                                                  ...                           \n",
      "2012-08-13  0.000000  0.000000  0.000000  0.000000  ...    0.0    0.0     0      0\n",
      "2012-08-14  0.000000  0.666667  0.000000  0.000000  ...    0.0    0.0     0      0\n",
      "2012-08-15  0.992069  0.994713  0.997356  0.992069  ...    0.0    0.0     0      0\n",
      "2012-08-16  0.436978  0.435200  0.434895  0.435178  ...    0.0    0.0     0      0\n",
      "2012-08-17  0.820683  0.823424  0.823108  0.823245  ...    0.0    0.0     0      1\n",
      "\n",
      "[5 rows x 78 columns]\n",
      "count=7019\n"
     ]
    }
   ],
   "source": [
    "!cp model/{model_name}_prep.py model/train\n",
    "!chmod 777 model/train\n",
    "!docker build -t {model_name}_prep .\n",
    "!docker run -v $(pwd)/local/$model_name:/opt/ml --rm {model_name}_prep train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalCount=7019\n",
      "trainCount=2807\n",
      "testCount=4212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>wvma2</th>\n",
       "      <th>wvma3</th>\n",
       "      <th>wvma4</th>\n",
       "      <th>wvma5</th>\n",
       "      <th>wvma6</th>\n",
       "      <th>wvma7</th>\n",
       "      <th>wvma8</th>\n",
       "      <th>wvma9</th>\n",
       "      <th>wvma10</th>\n",
       "      <th>...</th>\n",
       "      <th>roc9</th>\n",
       "      <th>roc10</th>\n",
       "      <th>roc11</th>\n",
       "      <th>roc12</th>\n",
       "      <th>roc13</th>\n",
       "      <th>roc14</th>\n",
       "      <th>roc15</th>\n",
       "      <th>roc16</th>\n",
       "      <th>long</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-12-01</th>\n",
       "      <td>0.969985</td>\n",
       "      <td>0.968653</td>\n",
       "      <td>0.969319</td>\n",
       "      <td>0.971962</td>\n",
       "      <td>0.975524</td>\n",
       "      <td>0.978269</td>\n",
       "      <td>0.980863</td>\n",
       "      <td>0.983606</td>\n",
       "      <td>0.986093</td>\n",
       "      <td>0.988215</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.199729</td>\n",
       "      <td>-5.026861</td>\n",
       "      <td>-1.668653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855746</td>\n",
       "      <td>1.580135</td>\n",
       "      <td>1.413645</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-12-02</th>\n",
       "      <td>0.962420</td>\n",
       "      <td>0.966082</td>\n",
       "      <td>0.967247</td>\n",
       "      <td>0.968611</td>\n",
       "      <td>0.971061</td>\n",
       "      <td>0.974222</td>\n",
       "      <td>0.976871</td>\n",
       "      <td>0.979424</td>\n",
       "      <td>0.982103</td>\n",
       "      <td>0.984585</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.756642</td>\n",
       "      <td>-5.264177</td>\n",
       "      <td>-6.082118</td>\n",
       "      <td>-2.761224</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>-0.264874</td>\n",
       "      <td>0.451467</td>\n",
       "      <td>0.286826</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-12-03</th>\n",
       "      <td>0.969183</td>\n",
       "      <td>0.967785</td>\n",
       "      <td>0.968917</td>\n",
       "      <td>0.969563</td>\n",
       "      <td>0.970515</td>\n",
       "      <td>0.972360</td>\n",
       "      <td>0.974889</td>\n",
       "      <td>0.977161</td>\n",
       "      <td>0.979431</td>\n",
       "      <td>0.981856</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.931879</td>\n",
       "      <td>-5.352330</td>\n",
       "      <td>-4.857751</td>\n",
       "      <td>-5.679202</td>\n",
       "      <td>-2.344060</td>\n",
       "      <td>-0.686869</td>\n",
       "      <td>0.162999</td>\n",
       "      <td>0.882413</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-12-04</th>\n",
       "      <td>0.830599</td>\n",
       "      <td>0.829635</td>\n",
       "      <td>0.828557</td>\n",
       "      <td>0.828727</td>\n",
       "      <td>0.828886</td>\n",
       "      <td>0.829343</td>\n",
       "      <td>0.830460</td>\n",
       "      <td>0.832131</td>\n",
       "      <td>0.833724</td>\n",
       "      <td>0.835378</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.194989</td>\n",
       "      <td>-5.606582</td>\n",
       "      <td>-5.025029</td>\n",
       "      <td>-4.528740</td>\n",
       "      <td>-5.353031</td>\n",
       "      <td>-2.006357</td>\n",
       "      <td>-0.343434</td>\n",
       "      <td>0.509372</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-12-05</th>\n",
       "      <td>0.972968</td>\n",
       "      <td>0.974104</td>\n",
       "      <td>0.974104</td>\n",
       "      <td>0.973569</td>\n",
       "      <td>0.973770</td>\n",
       "      <td>0.973960</td>\n",
       "      <td>0.974414</td>\n",
       "      <td>0.975490</td>\n",
       "      <td>0.977137</td>\n",
       "      <td>0.978766</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.380503</td>\n",
       "      <td>-4.525151</td>\n",
       "      <td>-5.931879</td>\n",
       "      <td>-5.352330</td>\n",
       "      <td>-4.857751</td>\n",
       "      <td>-5.679202</td>\n",
       "      <td>-2.344060</td>\n",
       "      <td>-0.686869</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               close     wvma2     wvma3     wvma4     wvma5     wvma6  \\\n",
       "dt                                                                       \n",
       "2003-12-01  0.969985  0.968653  0.969319  0.971962  0.975524  0.978269   \n",
       "2003-12-02  0.962420  0.966082  0.967247  0.968611  0.971061  0.974222   \n",
       "2003-12-03  0.969183  0.967785  0.968917  0.969563  0.970515  0.972360   \n",
       "2003-12-04  0.830599  0.829635  0.828557  0.828727  0.828886  0.829343   \n",
       "2003-12-05  0.972968  0.974104  0.974104  0.973569  0.973770  0.973960   \n",
       "\n",
       "               wvma7     wvma8     wvma9    wvma10  ...      roc9     roc10  \\\n",
       "dt                                                  ...                       \n",
       "2003-12-01  0.980863  0.983606  0.986093  0.988215  ... -4.199729 -5.026861   \n",
       "2003-12-02  0.976871  0.979424  0.982103  0.984585  ... -5.756642 -5.264177   \n",
       "2003-12-03  0.974889  0.977161  0.979431  0.981856  ... -5.931879 -5.352330   \n",
       "2003-12-04  0.830460  0.832131  0.833724  0.835378  ... -4.194989 -5.606582   \n",
       "2003-12-05  0.974414  0.975490  0.977137  0.978766  ... -3.380503 -4.525151   \n",
       "\n",
       "               roc11     roc12     roc13     roc14     roc15     roc16  long  \\\n",
       "dt                                                                             \n",
       "2003-12-01 -1.668653  0.000000  0.855746  1.580135  1.413645  0.671141     0   \n",
       "2003-12-02 -6.082118 -2.761224 -1.111111 -0.264874  0.451467  0.286826     1   \n",
       "2003-12-03 -4.857751 -5.679202 -2.344060 -0.686869  0.162999  0.882413     1   \n",
       "2003-12-04 -5.025029 -4.528740 -5.353031 -2.006357 -0.343434  0.509372     0   \n",
       "2003-12-05 -5.931879 -5.352330 -4.857751 -5.679202 -2.344060 -0.686869     1   \n",
       "\n",
       "            short  \n",
       "dt                 \n",
       "2003-12-01      0  \n",
       "2003-12-02      0  \n",
       "2003-12-03      0  \n",
       "2003-12-04      0  \n",
       "2003-12-05      0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"local/\"+model_name+\"/input/data/training/data.csv\", infer_datetime_format=True, parse_dates=['dt'], index_col=['dt'])\n",
    "print(\"totalCount=%s\" % len(df))\n",
    "\n",
    "trainCount=int(len(df)*0.4)\n",
    "dfTrain = df.iloc[:trainCount]\n",
    "dfTrain.to_csv(\"local/\"+model_name+\"/input/data/training/data_train.csv\")\n",
    "print(\"trainCount=%s\" % len(dfTrain))\n",
    "\n",
    "dfTest = df.iloc[trainCount:]\n",
    "dfTest.to_csv(\"local/\"+model_name+\"/input/data/training/data_test.csv\")\n",
    "print(\"testCount=%s\" % len(dfTest))\n",
    "dfTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3) Train the Model\n",
    "\n",
    "In the following cell, you can modify the model training code or leave it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/model_long_short_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/{model_name}.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "yLen=2\n",
    "b=0\n",
    "\n",
    "# Optional\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# These are the paths to where SageMaker mounts interesting things in your\n",
    "# container.\n",
    "prefix = '/opt/ml/'\n",
    "\n",
    "input_path = prefix + 'input/data/training/data_train.csv'\n",
    "test_path = prefix + 'input/data/training/data_test.csv'\n",
    "\n",
    "output_path = os.path.join(prefix, 'output')\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "\n",
    "# Process and prepare the data\n",
    "def data_process(df):\n",
    "    global yLen\n",
    "    global b\n",
    "    dataX=[]\n",
    "    dataY=[]\n",
    "    for idx,row in df.iterrows():\n",
    "        row1=[]\n",
    "        r=row[1:len(row)-yLen]\n",
    "        for a in r:\n",
    "            row1.append(a)\n",
    "        x=np.array(row1)\n",
    "        y=np.array(row[len(row)-yLen:])\n",
    "        b=len(x)\n",
    "        dataX.append(x)\n",
    "        dataY.append(y)\n",
    "    dataX=np.array(dataX).astype(np.float32)\n",
    "    dataY=np.array(dataY).astype(np.float32)\n",
    "    return dataX,dataY,b\n",
    "\n",
    "def build_classifier():\n",
    "    global b\n",
    "    global yLen\n",
    "    print(\"build_classifier:b=%s,yLen=%s\" % (b,yLen))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(b, input_dim=b, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(int(b/2), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(yLen,kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generate_model(dataX, dataY, b):\n",
    "    model=build_classifier()\n",
    "    model.fit(dataX, dataY, epochs=100, batch_size=1)\n",
    "    scores = model.evaluate(dataX, dataY, verbose=0)\n",
    "    print(\"Training Data %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    return model\n",
    "        \n",
    "def train():\n",
    "    print('Starting the training.')\n",
    "    try:\n",
    "        raw_data = pd.read_csv(input_path)\n",
    "        #print(raw_data)\n",
    "        X, y, b = data_process(raw_data)\n",
    "        model = generate_model(X, y, b)\n",
    "        model.save(os.path.join(model_path, 'model.h5'))\n",
    "        \n",
    "        print('Training is complete. Model saved.')\n",
    "        \n",
    "        raw_data = pd.read_csv(test_path)\n",
    "        testX, testY, b = data_process(raw_data)\n",
    "        scores = model.evaluate(testX, testY, verbose=0)\n",
    "        print(\"Test Data %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failure\n",
    "        # Reason in the DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\n' + trc)\n",
    "        # Printing this causes the exception to be in the training job logs\n",
    "        print(\n",
    "            'Exception during training: ' + str(e) + '\\n' + trc,\n",
    "            file=sys.stderr)\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n",
    "    # A zero exit code causes the job to be marked a Succeeded.\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                              TAG                 IMAGE ID            CREATED             SIZE\n",
      "173951039606.dkr.ecr.us-east-1.amazonaws.com/model_long_short_predict   latest              c1081251f6ff        About an hour ago   6.69GB\n",
      "model_long_short_predict                                                latest              c1081251f6ff        About an hour ago   6.69GB\n",
      "model_long_short_predict_prep                                           latest              3f5ffbe3f98a        About an hour ago   6.69GB\n",
      "173951039606.dkr.ecr.us-east-1.amazonaws.com/model_long_short_predict   <none>              cb35a7035fa3        About an hour ago   6.69GB\n",
      "<none>                                                                  <none>              9cf14168d9de        About an hour ago   6.69GB\n",
      "173951039606.dkr.ecr.us-east-1.amazonaws.com/model_long_short_predict   <none>              6699dce1d8c8        2 hours ago         6.69GB\n",
      "<none>                                                                  <none>              6a83b552f99e        2 hours ago         6.69GB\n",
      "tensorflow/tensorflow                                                   2.1.0rc2-py3        021062082fec        19 months ago       2.52GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Train Locally\n",
    "\n",
    "You can choose if you want to do the training locally (Option 1) or remote via SageMaker (Option 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  26.27MB\n",
      "Step 1/23 : FROM tensorflow/tensorflow:2.1.0rc2-py3\n",
      " ---> 021062082fec\n",
      "Step 2/23 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> c18207f35189\n",
      "Step 3/23 : RUN wget https://sourceforge.net/projects/ta-lib/files/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz && tar -xzf ta-lib-0.4.0-src.tar.gz && cd ta-lib/ && ./configure --prefix=/usr && make && make install && cd ../ && rm -rf ta-lib && rm ta-lib-0.4.0-src.tar.gz\n",
      " ---> Using cache\n",
      " ---> d057f1dca15b\n",
      "Step 4/23 : RUN wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py\n",
      " ---> Using cache\n",
      " ---> 09934ea62a85\n",
      "Step 5/23 : RUN pip install numpy\n",
      " ---> Using cache\n",
      " ---> 009ba349edbd\n",
      "Step 6/23 : RUN pip install scipy\n",
      " ---> Using cache\n",
      " ---> f0185befba4d\n",
      "Step 7/23 : RUN pip install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 2093f8f14efd\n",
      "Step 8/23 : RUN pip install pandas\n",
      " ---> Using cache\n",
      " ---> 138b2b6d5950\n",
      "Step 9/23 : RUN pip install flask\n",
      " ---> Using cache\n",
      " ---> a42de175add4\n",
      "Step 10/23 : RUN pip install gevent\n",
      " ---> Using cache\n",
      " ---> 8b8c4efda03f\n",
      "Step 11/23 : RUN pip install gunicorn\n",
      " ---> Using cache\n",
      " ---> c2c348814126\n",
      "Step 12/23 : RUN pip install tensorflow==2.2.0\n",
      " ---> Using cache\n",
      " ---> d41036bcf9a6\n",
      "Step 13/23 : RUN pip install tensorflow --upgrade\n",
      " ---> Using cache\n",
      " ---> 8184bbc7cf4c\n",
      "Step 14/23 : RUN pip install keras\n",
      " ---> Using cache\n",
      " ---> a9c2f1975416\n",
      "Step 15/23 : RUN pip install backtrader\n",
      " ---> Using cache\n",
      " ---> 467d97a879a0\n",
      "Step 16/23 : RUN pip install matplotlib==3.2.2\n",
      " ---> Using cache\n",
      " ---> 74f3436296f3\n",
      "Step 17/23 : RUN pip install ta-lib\n",
      " ---> Using cache\n",
      " ---> 65c5291307ad\n",
      "Step 18/23 : RUN pip install boto3\n",
      " ---> Using cache\n",
      " ---> cf4d1ef2deba\n",
      "Step 19/23 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 773c89f6e02a\n",
      "Step 20/23 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> b2027e7363d8\n",
      "Step 21/23 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 7ffa1693de58\n",
      "Step 22/23 : COPY model /opt/program\n",
      " ---> Using cache\n",
      " ---> 08e180624de9\n",
      "Step 23/23 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> c1081251f6ff\n",
      "Successfully built c1081251f6ff\n",
      "Successfully tagged model_long_short_predict:latest\n",
      "2021-08-16 20:52:09.498180: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-16 20:52:09.498216: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Starting the training.\n",
      "build_classifier:b=76,yLen=2\n",
      "2021-08-16 20:52:12.439918: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-16 20:52:12.439963: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-16 20:52:12.440022: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (85e9f28d2995): /proc/driver/nvidia/version does not exist\n",
      "2021-08-16 20:52:12.440580: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-16 20:52:12.661507: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/100\n",
      "2807/2807 [==============================] - 4s 1ms/step - loss: 0.5947 - accuracy: 0.6006\n",
      "Epoch 2/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5858 - accuracy: 0.6078\n",
      "Epoch 3/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5766 - accuracy: 0.6534\n",
      "Epoch 4/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5819 - accuracy: 0.6491\n",
      "Epoch 5/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5859 - accuracy: 0.5960\n",
      "Epoch 6/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5707 - accuracy: 0.6441\n",
      "Epoch 7/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5679 - accuracy: 0.6113\n",
      "Epoch 8/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5854 - accuracy: 0.5896\n",
      "Epoch 9/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5788 - accuracy: 0.6071\n",
      "Epoch 10/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5696 - accuracy: 0.5725\n",
      "Epoch 11/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5747 - accuracy: 0.5793\n",
      "Epoch 12/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5700 - accuracy: 0.6145\n",
      "Epoch 13/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5605 - accuracy: 0.5540\n",
      "Epoch 14/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5688 - accuracy: 0.5892\n",
      "Epoch 15/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5670 - accuracy: 0.6042\n",
      "Epoch 16/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5614 - accuracy: 0.5832\n",
      "Epoch 17/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5657 - accuracy: 0.5964\n",
      "Epoch 18/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5747 - accuracy: 0.5714\n",
      "Epoch 19/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5590 - accuracy: 0.6053\n",
      "Epoch 20/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5610 - accuracy: 0.5675\n",
      "Epoch 21/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5536 - accuracy: 0.5786\n",
      "Epoch 22/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5708 - accuracy: 0.5761\n",
      "Epoch 23/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5518 - accuracy: 0.5782\n",
      "Epoch 24/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5551 - accuracy: 0.5686\n",
      "Epoch 25/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5570 - accuracy: 0.5917\n",
      "Epoch 26/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5583 - accuracy: 0.5896\n",
      "Epoch 27/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5539 - accuracy: 0.5729\n",
      "Epoch 28/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5535 - accuracy: 0.5732\n",
      "Epoch 29/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5532 - accuracy: 0.5711\n",
      "Epoch 30/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5584 - accuracy: 0.5750\n",
      "Epoch 31/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5499 - accuracy: 0.5650\n",
      "Epoch 32/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5464 - accuracy: 0.5885\n",
      "Epoch 33/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5486 - accuracy: 0.5700\n",
      "Epoch 34/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5507 - accuracy: 0.6014\n",
      "Epoch 35/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5558 - accuracy: 0.5846\n",
      "Epoch 36/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5445 - accuracy: 0.5932\n",
      "Epoch 37/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5504 - accuracy: 0.5796\n",
      "Epoch 38/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5576 - accuracy: 0.6017\n",
      "Epoch 39/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5456 - accuracy: 0.6088\n",
      "Epoch 40/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5488 - accuracy: 0.5778\n",
      "Epoch 41/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5593 - accuracy: 0.5889\n",
      "Epoch 42/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5524 - accuracy: 0.5889\n",
      "Epoch 43/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5406 - accuracy: 0.5800\n",
      "Epoch 44/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5424 - accuracy: 0.5739\n",
      "Epoch 45/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5460 - accuracy: 0.5629\n",
      "Epoch 46/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5531 - accuracy: 0.5885\n",
      "Epoch 47/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5448 - accuracy: 0.6035\n",
      "Epoch 48/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5446 - accuracy: 0.5843\n",
      "Epoch 49/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5413 - accuracy: 0.5935\n",
      "Epoch 50/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5519 - accuracy: 0.5757\n",
      "Epoch 51/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5403 - accuracy: 0.5832\n",
      "Epoch 52/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5402 - accuracy: 0.5896\n",
      "Epoch 53/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5476 - accuracy: 0.6081\n",
      "Epoch 54/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5516 - accuracy: 0.5832\n",
      "Epoch 55/100\n",
      "2807/2807 [==============================] - 4s 1ms/step - loss: 0.5397 - accuracy: 0.5672\n",
      "Epoch 56/100\n",
      "2807/2807 [==============================] - 4s 1ms/step - loss: 0.5335 - accuracy: 0.5978\n",
      "Epoch 57/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5568 - accuracy: 0.5843\n",
      "Epoch 58/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5726 - accuracy: 0.5768\n",
      "Epoch 59/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5400 - accuracy: 0.5679\n",
      "Epoch 60/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5471 - accuracy: 0.5860\n",
      "Epoch 61/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5378 - accuracy: 0.5686\n",
      "Epoch 62/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5341 - accuracy: 0.5814\n",
      "Epoch 63/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5320 - accuracy: 0.5789\n",
      "Epoch 64/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5421 - accuracy: 0.5810\n",
      "Epoch 65/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5326 - accuracy: 0.5832\n",
      "Epoch 66/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5377 - accuracy: 0.5921\n",
      "Epoch 67/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5628 - accuracy: 0.5949\n",
      "Epoch 68/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5519 - accuracy: 0.5999\n",
      "Epoch 69/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5388 - accuracy: 0.5953\n",
      "Epoch 70/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5493 - accuracy: 0.5846\n",
      "Epoch 71/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5351 - accuracy: 0.5907\n",
      "Epoch 72/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5355 - accuracy: 0.5882\n",
      "Epoch 73/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5386 - accuracy: 0.5835\n",
      "Epoch 74/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5511 - accuracy: 0.6028\n",
      "Epoch 75/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5348 - accuracy: 0.6085\n",
      "Epoch 76/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5356 - accuracy: 0.5939\n",
      "Epoch 77/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5463 - accuracy: 0.6163\n",
      "Epoch 78/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5314 - accuracy: 0.5996\n",
      "Epoch 79/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5311 - accuracy: 0.6074\n",
      "Epoch 80/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5334 - accuracy: 0.5939\n",
      "Epoch 81/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5277 - accuracy: 0.5924\n",
      "Epoch 82/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5264 - accuracy: 0.6017\n",
      "Epoch 83/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5272 - accuracy: 0.6006\n",
      "Epoch 84/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5340 - accuracy: 0.5835\n",
      "Epoch 85/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5455 - accuracy: 0.6213\n",
      "Epoch 86/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5243 - accuracy: 0.5853\n",
      "Epoch 87/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5276 - accuracy: 0.5924\n",
      "Epoch 88/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5673 - accuracy: 0.5974\n",
      "Epoch 89/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5227 - accuracy: 0.6053\n",
      "Epoch 90/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5257 - accuracy: 0.5871\n",
      "Epoch 91/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5313 - accuracy: 0.5843\n",
      "Epoch 92/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5306 - accuracy: 0.5753\n",
      "Epoch 93/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5281 - accuracy: 0.5739\n",
      "Epoch 94/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5243 - accuracy: 0.5946\n",
      "Epoch 95/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5476 - accuracy: 0.5554\n",
      "Epoch 96/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5126 - accuracy: 0.5957\n",
      "Epoch 97/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5305 - accuracy: 0.5729\n",
      "Epoch 98/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5229 - accuracy: 0.5953\n",
      "Epoch 99/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5193 - accuracy: 0.5850\n",
      "Epoch 100/100\n",
      "2807/2807 [==============================] - 3s 1ms/step - loss: 0.5387 - accuracy: 0.6095\n",
      "Training Data accuracy: 59.07%\n",
      "Training is complete. Model saved.\n",
      "Test Data accuracy: 47.13%\n"
     ]
    }
   ],
   "source": [
    "# Build Local ML Image\n",
    "!cp model/{model_name}.py model/train\n",
    "!chmod 777 model/train\n",
    "!docker build -t {model_name} .\n",
    "!docker run -v $(pwd)/local/$model_name:/opt/ml --rm {model_name} train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Model Artifact to Strategies Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 139232 Aug 16 20:57 local/model_long_short_predict/model/model.h5\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 139232 Aug 16 20:57 ../2_Strategies/model/model_long_short_predict.h5\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  32544 Aug 12 16:54 ../2_Strategies/model/model_long_short_predict_intraday.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -la local/{model_name}/model/model.h5\n",
    "!cp local/{model_name}/model/model.h5 ../2_Strategies/model/{model_name}.h5\n",
    "!ls -la ../2_Strategies/model/model_*.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Remote Training via SageMaker\n",
    "\n",
    "You can choose if you want to do the training locally (Option 1) or remote via SageMaker (Option 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon   26.3MB\n",
      "Step 1/23 : FROM tensorflow/tensorflow:2.1.0rc2-py3\n",
      " ---> 021062082fec\n",
      "Step 2/23 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> c18207f35189\n",
      "Step 3/23 : RUN wget https://sourceforge.net/projects/ta-lib/files/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz && tar -xzf ta-lib-0.4.0-src.tar.gz && cd ta-lib/ && ./configure --prefix=/usr && make && make install && cd ../ && rm -rf ta-lib && rm ta-lib-0.4.0-src.tar.gz\n",
      " ---> Using cache\n",
      " ---> d057f1dca15b\n",
      "Step 4/23 : RUN wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py\n",
      " ---> Using cache\n",
      " ---> 09934ea62a85\n",
      "Step 5/23 : RUN pip install numpy\n",
      " ---> Using cache\n",
      " ---> 009ba349edbd\n",
      "Step 6/23 : RUN pip install scipy\n",
      " ---> Using cache\n",
      " ---> f0185befba4d\n",
      "Step 7/23 : RUN pip install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 2093f8f14efd\n",
      "Step 8/23 : RUN pip install pandas\n",
      " ---> Using cache\n",
      " ---> 138b2b6d5950\n",
      "Step 9/23 : RUN pip install flask\n",
      " ---> Using cache\n",
      " ---> a42de175add4\n",
      "Step 10/23 : RUN pip install gevent\n",
      " ---> Using cache\n",
      " ---> 8b8c4efda03f\n",
      "Step 11/23 : RUN pip install gunicorn\n",
      " ---> Using cache\n",
      " ---> c2c348814126\n",
      "Step 12/23 : RUN pip install tensorflow==2.2.0\n",
      " ---> Using cache\n",
      " ---> d41036bcf9a6\n",
      "Step 13/23 : RUN pip install tensorflow --upgrade\n",
      " ---> Using cache\n",
      " ---> 8184bbc7cf4c\n",
      "Step 14/23 : RUN pip install keras\n",
      " ---> Using cache\n",
      " ---> a9c2f1975416\n",
      "Step 15/23 : RUN pip install backtrader\n",
      " ---> Using cache\n",
      " ---> 467d97a879a0\n",
      "Step 16/23 : RUN pip install matplotlib==3.2.2\n",
      " ---> Using cache\n",
      " ---> 74f3436296f3\n",
      "Step 17/23 : RUN pip install ta-lib\n",
      " ---> Using cache\n",
      " ---> 65c5291307ad\n",
      "Step 18/23 : RUN pip install boto3\n",
      " ---> Using cache\n",
      " ---> cf4d1ef2deba\n",
      "Step 19/23 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 773c89f6e02a\n",
      "Step 20/23 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> b2027e7363d8\n",
      "Step 21/23 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 7ffa1693de58\n",
      "Step 22/23 : COPY model /opt/program\n",
      " ---> Using cache\n",
      " ---> 08e180624de9\n",
      "Step 23/23 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> c1081251f6ff\n",
      "Successfully built c1081251f6ff\n",
      "Successfully tagged model_long_short_predict:latest\n",
      "The push refers to repository [173951039606.dkr.ecr.us-east-1.amazonaws.com/model_long_short_predict]\n",
      "\n",
      "\u001b[1Bf07abe7a: Preparing \n",
      "\u001b[1Bd4c08dc5: Preparing \n",
      "\u001b[1B62a96903: Preparing \n",
      "\u001b[1Bc7617abe: Preparing \n",
      "\u001b[1B40044990: Preparing \n",
      "\u001b[1Bc9a4c5b5: Preparing \n",
      "\u001b[1B4969d9f1: Preparing \n",
      "\u001b[1Be4d1131e: Preparing \n",
      "\u001b[1B0963839f: Preparing \n",
      "\u001b[1B991ae1f9: Preparing \n",
      "\u001b[1B83534df3: Preparing \n",
      "\u001b[1Bc7c06055: Preparing \n",
      "\u001b[1Bbf6661e7: Preparing \n",
      "\u001b[1Bad534aeb: Preparing \n",
      "\u001b[1B70ff380b: Preparing \n",
      "\u001b[1B4d0d28d5: Preparing \n",
      "\u001b[1Bf0c21d0e: Preparing \n",
      "\u001b[1Bf64f5098: Preparing \n",
      "\u001b[1B8bd7c2a2: Preparing \n",
      "\u001b[1Bbe37ceb9: Preparing \n",
      "\u001b[1B7e49cdd8: Preparing \n",
      "\u001b[1Bfe0f938f: Preparing \n",
      "\u001b[1B71112be5: Preparing \n",
      "\u001b[1Ba0a694d8: Preparing \n",
      "\u001b[1B901684b5: Preparing \n",
      "\u001b[1Bfb8f161b: Preparing \n",
      "\u001b[1B43ea46a8: Preparing \n",
      "\u001b[1Bfcc4a1a8: Preparing \n",
      "\u001b[21B963839f: Layer already exists \u001b[24A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[21A\u001b[2Klatest: digest: sha256:e082e4a5edf81215008b652907a185461cd127f9a729d29bdec727c7bdee0859 size: 6414\n"
     ]
    }
   ],
   "source": [
    "# Deploy ML Image to ECS\n",
    "!./build_and_push.sh $model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-173951039606/data\n",
      "{}\n",
      "2021-08-16 20:57:48 Starting - Starting the training job...\n",
      "2021-08-16 20:58:11 Starting - Launching requested ML instancesProfilerReport-1629147467: InProgress\n",
      "......\n",
      "2021-08-16 20:59:11 Starting - Preparing the instances for training.........\n",
      "2021-08-16 21:00:48 Downloading - Downloading input data\n",
      "2021-08-16 21:00:48 Training - Downloading the training image.......\n",
      "2021-08-16 21:09:54 Uploading - Uploading generated training model\n",
      "2021-08-16 21:09:54 Completed - Training job completed\n",
      "ProfilerReport-1629147467: NoIssuesFound\n",
      "Training seconds: 552\n",
      "Billable seconds: 552\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import json\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sage.Session()\n",
    "\n",
    "WORK_DIRECTORY = 'local/'+model_name+'/input/data/training'\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix='data')\n",
    "print(data_location)\n",
    "\n",
    "conf_file='local/'+model_name+'/input/config/hyperparameters.json'\n",
    "with open(conf_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "print(config)\n",
    "\n",
    "prefix=model_name\n",
    "job_name=prefix.replace('_','-')\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{prefix}:latest'\n",
    "\n",
    "classifier = sage.estimator.Estimator(\n",
    "    image_uri=image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name=job_name)\n",
    "classifier.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Model Artifact from Amazon S3 and copy it to Strategies Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Model from S3\n",
    "model_name_s3=classifier.model_data.replace('s3://'+sess.default_bucket()+'/','')\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(sess.default_bucket())\n",
    "my_bucket.download_file(model_name_s3,'model.tar.gz')\n",
    "!tar -xzf model.tar.gz\n",
    "!rm model.tar.gz\n",
    "!cp model.h5 ../2_Strategies/model/{model_name}.h5\n",
    "!ls -la model.h5\n",
    "!ls -la ../2_Strategies/model/model_*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
